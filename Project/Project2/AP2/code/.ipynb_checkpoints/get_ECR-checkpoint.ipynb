{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "import argparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import mdptoolbox, mdptoolbox.example\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "# load data set with selected or extracted features, features are discrete\n",
    "# features are the columns after reward column\n",
    "def generate_MDP_input(filename):\n",
    "    original_data = pandas.read_csv(filename)\n",
    "    feature_name = list(original_data)\n",
    "    reward_idx = feature_name.index('reward')\n",
    "    start_Fidx = reward_idx + 1\n",
    "\n",
    "    students_variables = ['student', 'priorTutorAction', 'reward']\n",
    "    features = feature_name[start_Fidx: len(feature_name)]\n",
    "\n",
    "    # generate distinct state based on feature\n",
    "    original_data['state'] = original_data[features].apply(lambda x: ':'.join(str(v) for v in x), axis=1)\n",
    "    # original_data['state'] = original_data[features].apply(tuple, axis=1)\n",
    "    students_variables = students_variables + ['state']\n",
    "    data = original_data[students_variables]\n",
    "\n",
    "    # quantify actions\n",
    "    distinct_acts = ['PS', 'WE']\n",
    "    Nx = len(distinct_acts)\n",
    "    i = 0\n",
    "    for act in distinct_acts:\n",
    "        data.loc[data['priorTutorAction'] == act, 'priorTutorAction'] = i\n",
    "        i += 1\n",
    "\n",
    "    # initialize state transition table, expected reward table, starting state table\n",
    "    # distinct_states didn't contain terminal state\n",
    "    student_list = list(data['student'].unique())\n",
    "    distinct_states = list()\n",
    "    for student in student_list:\n",
    "        student_data = data.loc[data['student'] == student,]\n",
    "        # don't consider last row\n",
    "        temp_states = list(student_data['state'])[0:-1]\n",
    "        distinct_states = distinct_states + temp_states\n",
    "    distinct_states = list(set(distinct_states))\n",
    "\n",
    "    Ns = len(distinct_states)\n",
    "\n",
    "    # we include terminal state\n",
    "    start_states = np.zeros(Ns + 1)\n",
    "    A = np.zeros((Nx, Ns + 1, Ns + 1))\n",
    "    expectR = np.zeros((Nx, Ns + 1, Ns + 1))\n",
    "\n",
    "    # update table values episode by episode\n",
    "    # each episode is a student data set\n",
    "    for student in student_list:\n",
    "        student_data = data.loc[data['student'] == student,]\n",
    "        row_list = student_data.index.tolist()\n",
    "\n",
    "        # count the number of start state\n",
    "        start_states[distinct_states.index(student_data.loc[row_list[0], 'state'])] += 1\n",
    "\n",
    "        # count the number of transition among states without terminal state\n",
    "        for i in range(1, (len(row_list) - 1)):\n",
    "            state1 = distinct_states.index(student_data.loc[row_list[i - 1], 'state'])\n",
    "            state2 = distinct_states.index(student_data.loc[row_list[i], 'state'])\n",
    "            act = student_data.loc[row_list[i], 'priorTutorAction']\n",
    "\n",
    "            # count the state transition\n",
    "            A[act, state1, state2] += 1\n",
    "            expectR[act, state1, state2] += float(student_data.loc[row_list[i], 'reward'])\n",
    "\n",
    "        # count the number of transition from state to terminal\n",
    "        state1 = distinct_states.index(student_data.loc[row_list[-2], 'state'])\n",
    "        act = student_data.loc[row_list[-1], 'priorTutorAction']\n",
    "        A[act, state1, Ns] += 1\n",
    "        expectR[act, state1, Ns] += float(student_data.loc[row_list[-1], 'reward'])\n",
    "\n",
    "    # normalization\n",
    "    start_states = start_states / np.sum(start_states)\n",
    "\n",
    "    for act in range(Nx):\n",
    "        A[act, Ns, Ns] = 1\n",
    "        # generate expected reward\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            expectR[act] = np.divide(expectR[act], A[act])\n",
    "            expectR[np.isnan(expectR)] = 0\n",
    "\n",
    "        # each column will sum to 1 for each row, obtain the state transition table\n",
    "        # some states only have either PS or WE transition to other state\n",
    "        for l in np.where(np.sum(A[act], axis=1) == 0)[0]:\n",
    "            A[act, l, l] = 1\n",
    "\n",
    "        A[act] = np.divide(A[act].transpose(), np.sum(A[act], axis=1))\n",
    "        A[act] = A[act].transpose()\n",
    "\n",
    "    return [start_states, A, expectR, distinct_acts, distinct_states]\n",
    "\n",
    "\n",
    "def calcuate_ECR(start_states, expectV):\n",
    "        ECR_value = start_states.dot(np.array(expectV))\n",
    "        return ECR_value\n",
    "\n",
    "def output_policy(distinct_acts, distinct_states, vi):\n",
    "    Ns = len(distinct_states)\n",
    "    print('Policy: ')\n",
    "    print('state -> action, value-function')\n",
    "    for s in range(Ns):\n",
    "        print(distinct_states[s]+ \" -> \" + distinct_acts[vi.policy[s]] + \", \"+str(vi.V[s]))\n",
    "\n",
    "\n",
    "def calcuate_Q(T, R, V, gamma):\n",
    "    q = np.zeros((T.shape[1], T.shape[0]))\n",
    "\n",
    "    for s in range(T.shape[1]):\n",
    "        for a in range(T.shape[0]):\n",
    "            r = np.dot(R[a][s], T[a][s])\n",
    "            Q = r + gamma * np.dot(T[a][s], V)\n",
    "            q[s][a] = Q\n",
    "\n",
    "    return q\n",
    "\n",
    "def output_Qvalue(distinct_acts, distinct_states, Q):\n",
    "    Ns = len(distinct_states)\n",
    "    Na = len(distinct_acts)\n",
    "    print('Q-value in Policy: ')\n",
    "    print('state -> action, Q value function')\n",
    "    for s in range(Ns):\n",
    "        for a in range(Na):\n",
    "            print(distinct_states[s] + \" -> \" + distinct_acts[a] + \", \" + str(Q[s][a]))\n",
    "\n",
    "\n",
    "def calculate_IS(filename, distinct_acts, distinct_states, Q, gamma, theta):\n",
    "    original_data = pandas.read_csv(filename)\n",
    "    feature_name = list(original_data)\n",
    "    reward_idx = feature_name.index('reward')\n",
    "    start_Fidx = reward_idx + 1\n",
    "\n",
    "    students_variables = ['student', 'priorTutorAction', 'reward']\n",
    "    features = feature_name[start_Fidx: len(feature_name)]\n",
    "\n",
    "    # generate distinct state based on feature\n",
    "    original_data['state'] = original_data[features].apply(lambda x: ':'.join(str(v) for v in x), axis=1)\n",
    "    # original_data['state'] = original_data[features].apply(tuple, axis=1)\n",
    "    students_variables = students_variables + ['state']\n",
    "    data = original_data[students_variables]\n",
    "\n",
    "    i = 0\n",
    "    for act in distinct_acts:\n",
    "        data.loc[data['priorTutorAction'] == act, 'priorTutorAction'] = i\n",
    "        i += 1\n",
    "\n",
    "\n",
    "    IS = 0\n",
    "    random_prob = 0.5\n",
    "\n",
    "    student_list = list(data['student'].unique())\n",
    "    for student in student_list:\n",
    "        student_data = data.loc[data['student'] == student,]\n",
    "        row_list = student_data.index.tolist()\n",
    "\n",
    "\n",
    "        cumul_policy_prob = 0\n",
    "        cumul_random_prob = 0\n",
    "        cumulative_reward = 0\n",
    "\n",
    "        # calculate Importance Sampling Value for single student\n",
    "        for i in range(1, len(row_list)):\n",
    "            state = distinct_states.index(student_data.loc[row_list[i - 1], 'state'])\n",
    "            action = student_data.loc[row_list[i], 'priorTutorAction']\n",
    "            reward = float(student_data.loc[row_list[i], 'reward'])\n",
    "\n",
    "            Q_PS = Q[state][0]\n",
    "            Q_WE = Q[state][1]\n",
    "\n",
    "            #\n",
    "            diff = Q_PS - Q_WE\n",
    "            if diff > 60:\n",
    "                diff = 60\n",
    "            if diff < -60:\n",
    "                diff = -60\n",
    "\n",
    "            if action == 0:  # PS\n",
    "                prob_logP = 1 / (1 + math.exp(-diff * theta))\n",
    "            else:  # WE\n",
    "                prob_logP = 1 / (1 + math.exp(diff * theta))\n",
    "\n",
    "\n",
    "            cumul_policy_prob += math.log(prob_logP)\n",
    "            cumul_random_prob += math.log(random_prob)\n",
    "            cumulative_reward += math.pow(gamma, i-1) * reward\n",
    "            i += 1\n",
    "\n",
    "        weight = np.exp(cumul_policy_prob - cumul_random_prob)\n",
    "        IS_reward = cumulative_reward * weight\n",
    "\n",
    "        # cap the IS value\n",
    "        if IS_reward > 300:\n",
    "            IS_reward = 300\n",
    "        if IS_reward < -300:\n",
    "            IS_reward = -300\n",
    "        IS += IS_reward\n",
    "\n",
    "    IS = float(IS) / len(student_list)\n",
    "    return IS\n",
    "\n",
    "def compute_ECR(filename):\n",
    "\n",
    "\n",
    "    # load data set with selected or extracted discrete features\n",
    "    [start_states, A, expectR, distinct_acts, distinct_states] = generate_MDP_input(filename)\n",
    "\n",
    "    # apply Value Iteration to run the MDP\n",
    "    vi = mdptoolbox.mdp.ValueIteration(A, expectR, discount = 0.9)\n",
    "    vi.run()\n",
    "\n",
    "    # output policy\n",
    "    #output_policy(distinct_acts, distinct_states, vi)\n",
    "\n",
    "    # evaluate policy using ECR\n",
    "    ECR_value = calcuate_ECR(start_states, vi.V)\n",
    "    #print('ECR value: ' + str(ECR_value))\n",
    "\n",
    "\n",
    "    # calculate Q-value based on MDP\n",
    "    Q = calcuate_Q(A, expectR, vi.V, 0.9)\n",
    "\n",
    "    # output Q-value for each state-action pair\n",
    "    # output_Qvalue(distinct_acts, distinct_states, Q)\n",
    "\n",
    "    # evaluate policy using Importance Sampling\n",
    "    IS_value = calculate_IS(filename, distinct_acts, distinct_states, Q, 0.9, 0.1)\n",
    "    print('IS value: ' + str(IS_value))\n",
    "    return (ECR_value, IS_value)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
